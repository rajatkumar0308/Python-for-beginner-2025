{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d784687b-e4f6-46f9-828e-20eae3d60f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "The purpose of this code is to perform stress testing on a mortgage portfolio to evaluate how it would perform under adverse economic conditions. Specifically:\n",
    "\n",
    "Modeling Default Risk: It simulates how changes in external factors (like unemployment, interest rates, and property values) impact the default probability (PD) of loans in the portfolio.\n",
    "\n",
    "Assessing Losses: It calculates the Loss Given Default (LGD) based on changes in property values (through the loan-to-value ratio) and uses this to estimate the Expected Loss (EL) for each loan.\n",
    "\n",
    "Portfolio Risk Evaluation: Finally, it aggregates the individual loan losses to determine the total portfolio loss under the simulated stress scenarios, helping to assess the overall risk exposure of the portfolio under extreme but plausible conditions.\n",
    "\n",
    "'''\n",
    "Step 1: Load Mortgage Portfolio Data\n",
    "A Spark session is created to process the data. Sample mortgage data is defined and converted into a PySpark DataFrame for analysis.\n",
    "\n",
    "Step 2: Define Stress Scenarios\n",
    "Stress parameters are defined to simulate shocks, including a 10% increase in unemployment, a 2% rise in interest rates, and a 20% drop in property values.\n",
    "\n",
    "Step 3: Model Default Probability (PD)\n",
    "A base PD is calculated based on credit score using a logistic function, which is then adjusted for stress scenarios (e.g., increased unemployment).\n",
    "\n",
    "Step 4: Calculate LGD and EAD\n",
    "The stressed property value is computed by applying the 20% decline, and the loan-to-value (LTV) ratio is recalculated. The Loss Given Default (LGD) is then determined based on the new LTV.\n",
    "\n",
    "Step 5: Compute Expected Loss (EL)\n",
    "Expected Loss (EL) is calculated for each loan by multiplying the stress-adjusted PD, LGD, and loan balance (Exposure at Default or EAD).\n",
    "\n",
    "Step 6: Aggregate Portfolio Loss\n",
    "The total portfolio loss is computed by summing the expected losses across all loans, providing a measure of portfolio risk under the stressed scenario.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d55f97-dc7b-4b3c-9ae4-a8b96686c86f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+----+------------+--------------+\n|loan_id|loan_balance|interest_rate| ltv|credit_score|property_value|\n+-------+------------+-------------+----+------------+--------------+\n|      1|      500000|         0.04|0.75|         700|        300000|\n|      2|      750000|        0.035|0.85|         680|        600000|\n|      3|      300000|         0.05|0.65|         720|        200000|\n+-------+------------+-------------+----+------------+--------------+\n\n+-------+------------------+\n|loan_id|     expected_loss|\n+-------+------------------+\n|      1|160828.40236686394|\n|      2|213061.88418916863|\n|      3| 105885.2326183207|\n+-------+------------------+\n\nTotal portfolio loss under stress scenario: $479,775.52\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit \n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Step 1: Load Mortgage Portfolio Data\n",
    "spark = SparkSession.builder.appName(\"StressTesting\").getOrCreate()\n",
    "\n",
    "# Sample mortgage data\n",
    "data = [\n",
    "    (1, 500000, 0.04, 0.75, 700, 300000),\n",
    "    (2, 750000, 0.035, 0.85, 680, 600000),\n",
    "    (3, 300000, 0.05, 0.65, 720, 200000)\n",
    "]\n",
    "columns = [\"loan_id\", \"loan_balance\", \"interest_rate\", \"ltv\", \"credit_score\", \"property_value\"]\n",
    "\n",
    "# Create DataFrame\n",
    "mortgage_df = spark.createDataFrame(data, columns)\n",
    "mortgage_df.show()\n",
    "\n",
    "# Step 2: Define Stress Scenarios\n",
    "stress_params = {\n",
    "    \"unemployment_increase\": 0.10,  # 10% rise\n",
    "    \"interest_rate_shock\": 0.02,    # +2%\n",
    "    \"property_value_decline\": 0.20  # -20%\n",
    "}\n",
    "\n",
    "# Step 3: Model Default Probability (PD)\n",
    "\n",
    "# Base PD based on credit score using a logistic function\n",
    "def calculate_base_pd(credit_score):\n",
    "    return 1 / (1 + (2.718 ** (-0.02 * (credit_score - 650))))  # Logistic function\n",
    "\n",
    "# Stress-adjusted PD function\n",
    "def stress_pd(base_pd, unemployment_shock):\n",
    "    return base_pd * (1 + unemployment_shock)\n",
    "\n",
    "# Register UDFs\n",
    "calculate_base_pd_udf = udf(calculate_base_pd, DoubleType())\n",
    "stress_pd_udf = udf(stress_pd, DoubleType())\n",
    "\n",
    "# Apply to DataFrame\n",
    "mortgage_df = mortgage_df.withColumn(\"base_pd\", calculate_base_pd_udf(col(\"credit_score\")))\n",
    "mortgage_df = mortgage_df.withColumn(\"stress_pd\", stress_pd_udf(col(\"base_pd\"), lit(stress_params[\"unemployment_increase\"])))\n",
    "\n",
    "# Step 4: Calculate LGD and EAD\n",
    "# Stressed property values and recalculated LTV ratio\n",
    "mortgage_df = mortgage_df.withColumn(\"stressed_property_value\", col(\"property_value\") * (1 - stress_params[\"property_value_decline\"]))\n",
    "mortgage_df = mortgage_df.withColumn(\"stressed_ltv\", col(\"loan_balance\") / col(\"stressed_property_value\"))\n",
    "\n",
    "# LGD model: 40% if LTV > 80%, else 25%\n",
    "mortgage_df = mortgage_df.withColumn(\"lgd\", when(col(\"stressed_ltv\") > 0.8, 0.4).otherwise(0.25))\n",
    "\n",
    "# EAD = loan balance (same as original balance)\n",
    "mortgage_df = mortgage_df.withColumn(\"ead\", col(\"loan_balance\"))\n",
    "\n",
    "# Step 5: Compute Expected Loss (EL)\n",
    "mortgage_df = mortgage_df.withColumn(\"expected_loss\", col(\"stress_pd\") * col(\"lgd\") * col(\"ead\"))\n",
    "mortgage_df.select(\"loan_id\", \"expected_loss\").show()\n",
    "\n",
    "# Step 6: Aggregate Portfolio Loss\n",
    "total_loss = mortgage_df.agg({\"expected_loss\": \"sum\"}).collect()[0][0]\n",
    "print(f\"Total portfolio loss under stress scenario: ${total_loss:,.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "STRESS TESTING",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
