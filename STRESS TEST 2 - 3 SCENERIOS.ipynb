{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed153e3c-16c7-4198-9e27-9cb798cc10b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#stress scenarios (baseline, adverse, and extreme) \n",
    "\n",
    "'''\n",
    "Define Mortgage Data: Import and create a DataFrame that holds sample mortgage data, including loan balances, interest rates, loan-to-value (LTV) ratios, credit scores, and property values.\n",
    "\n",
    "Define Scenarios: Create different economic stress scenarios (e.g., baseline, adverse, extreme) that apply varying levels of unemployment increase and property value decline.\n",
    "\n",
    "Model Losses under Stress: For each scenario, apply adjustments to the mortgage data (e.g., adjusting property values, recalculating LTV ratios, and increasing the probability of default) to simulate the effects of the stress scenario on the loans.\n",
    "\n",
    "Calculate Expected Losses: For each loan, calculate the expected loss based on the adjusted data (using parameters such as stress-tested property values, probability of default, and loss given default).\n",
    "\n",
    "Aggregate Results: Aggregate the expected losses across all loans for each scenario to calculate the total financial loss for each scenario.\n",
    "\n",
    "Store and Display Results: Store the total loss for each scenario in a DataFrame and display the results to help understand the impact of different stress conditions on the mortgage portfolio'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248eebd2-7aea-4cb8-a5ac-0f89c53506d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n|scenario|        total_loss|\n+--------+------------------+\n|baseline| 436159.5628857756|\n| adverse|479775.51917435334|\n| extreme| 523391.4754629308|\n+--------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "# Importing required types for defining the schema of the DataFrame.\n",
    "\n",
    "# Define schema for scenario results\n",
    "results_schema = StructType([ \n",
    "    StructField(\"scenario\", StringType(), nullable=False),  # Defines the 'scenario' field of type String\n",
    "    StructField(\"total_loss\", DoubleType(), nullable=False)  # Defines the 'total_loss' field of type Double\n",
    "])\n",
    "\n",
    "# Initialize empty DataFrame with the schema\n",
    "results_df = spark.createDataFrame([], schema=results_schema) \n",
    "# Creates an empty DataFrame 'results_df' with the defined schema 'results_schema'.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, udf\n",
    "from pyspark.sql.types import DoubleType, StringType, StructType, StructField\n",
    "# Importing additional necessary functions and types from PySpark.\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MultiScenarioAnalysis\").getOrCreate() \n",
    "# Initializes a Spark session, which is necessary for creating and processing DataFrames.\n",
    "\n",
    "# Sample mortgage data\n",
    "data = [\n",
    "    (1, 500000, 0.04, 0.75, 700, 300000),  # Sample data for mortgage (loan_id, loan_balance, interest_rate, ltv, credit_score, property_value)\n",
    "    (2, 750000, 0.035, 0.85, 680, 600000), \n",
    "    (3, 300000, 0.05, 0.65, 720, 200000)\n",
    "]\n",
    "columns = [\"loan_id\", \"loan_balance\", \"interest_rate\", \"ltv\", \"credit_score\", \"property_value\"]\n",
    "mortgage_df = spark.createDataFrame(data, columns) \n",
    "# Creates a DataFrame 'mortgage_df' from the sample data and assigns appropriate column names.\n",
    "\n",
    "# Define scenarios\n",
    "scenarios = {\n",
    "    \"baseline\": {\"unemployment_increase\": 0.0, \"property_value_decline\": 0.0},  # Baseline scenario (no change)\n",
    "    \"adverse\": {\"unemployment_increase\": 0.10, \"property_value_decline\": 0.15},  # Adverse scenario (with certain stress factors)\n",
    "    \"extreme\": {\"unemployment_increase\": 0.20, \"property_value_decline\": 0.30}  # Extreme scenario (greater stress factors)\n",
    "}\n",
    "\n",
    "# Define UDF for base PD calculation\n",
    "@udf(DoubleType())  # Declares a User Defined Function (UDF) to calculate base Probability of Default (PD)\n",
    "def calculate_base_pd(credit_score):\n",
    "    return 1 / (1 + (2.718 ** (-0.02 * (credit_score - 650)))) \n",
    "    # The UDF calculates the base PD based on a credit score using the logistic function.\n",
    "\n",
    "# Initialize results DataFrame with explicit schema\n",
    "results_schema = StructType([ \n",
    "    StructField(\"scenario\", StringType(), nullable=False), \n",
    "    StructField(\"total_loss\", DoubleType(), nullable=False) \n",
    "]) \n",
    "# Re-defining the schema for results (it was already defined earlier, but this line restates it).\n",
    "\n",
    "results_df = spark.createDataFrame([], schema=results_schema) \n",
    "# Re-initializes an empty DataFrame 'results_df' to store the results of each scenario.\n",
    "\n",
    "# Iterate through scenarios\n",
    "for scenario_name, params in scenarios.items():  # Iterating over each defined scenario (baseline, adverse, extreme)\n",
    "    # Apply scenario parameters\n",
    "    mortgage_stressed = mortgage_df \\\n",
    "        .withColumn(\"stressed_property_value\", col(\"property_value\") * (1 - params[\"property_value_decline\"])) \\\n",
    "        .withColumn(\"stressed_ltv\", col(\"loan_balance\") / col(\"stressed_property_value\")) \\\n",
    "        .withColumn(\"lgd\", when(col(\"stressed_ltv\") > 0.8, 0.4).otherwise(0.25)) \\\n",
    "        .withColumn(\"base_pd\", calculate_base_pd(col(\"credit_score\"))) \\\n",
    "        .withColumn(\"stress_pd\", col(\"base_pd\") * (1 + params[\"unemployment_increase\"])) \\\n",
    "        .withColumn(\"expected_loss\", col(\"stress_pd\") * col(\"lgd\") * col(\"loan_balance\"))\n",
    "    # The code creates new columns based on the scenario parameters, including 'stressed_property_value', \n",
    "    # 'stressed_ltv', 'lgd' (Loss Given Default), 'base_pd' (base Probability of Default), 'stress_pd' (stressed PD), \n",
    "    # and 'expected_loss' for each mortgage in the dataset.\n",
    "\n",
    "    # Calculate total loss for the scenario\n",
    "    total_loss = mortgage_stressed.agg({\"expected_loss\": \"sum\"}).collect()[0][0]\n",
    "    # Sums the 'expected_loss' column across all mortgages to get the total loss for the scenario.\n",
    "\n",
    "    # Append results to results_df\n",
    "    new_row = spark.createDataFrame(\n",
    "        [(scenario_name, float(total_loss))],  # Creates a new row for the result\n",
    "        schema=results_schema  # Ensures the new row follows the defined schema\n",
    "    )\n",
    "    results_df = results_df.union(new_row)  # Appends the new row to the results DataFrame.\n",
    "\n",
    "# Show results\n",
    "results_df.show()  # Displays the final DataFrame containing the total loss for each scenario.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "STRESS TEST 2 - 3 SCENERIOS",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
