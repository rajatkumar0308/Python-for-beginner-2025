{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "734d3f0b-0bb7-47a7-a690-a16108571729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------+------+------------+-------------+----+---------+\n|Customer_ID|Credit_Score|Loan_Amount|Income|Default_Flag|Interest_Rate| LTV|Loan_Term|\n+-----------+------------+-----------+------+------------+-------------+----+---------+\n|          1|         750|     300000|  5000|           1|         0.03| 0.7|      0.9|\n|          2|         650|     150000|  3000|           0|         0.05| 0.6|      0.8|\n|          3|         680|     250000|  4000|           1|         0.04|0.65|     0.85|\n|          4|         620|     100000|  2000|           0|          0.1| 0.5|      0.7|\n|          5|         700|     400000|  6000|           1|         0.02|0.75|     0.95|\n+-----------+------------+-----------+------+------------+-------------+----+---------+\n\n+-----------+----------+--------------------+\n|Customer_ID|prediction|         probability|\n+-----------+----------+--------------------+\n|          1|       1.0|[2.33146222098297...|\n+-----------+----------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4214444101529135>:96\u001B[0m\n",
       "\u001B[1;32m     92\u001B[0m predictions\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCustomer_ID\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprobability\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# Step 7: Extract the Probability of Default (PD) from the 'probability' column\u001B[39;00m\n",
       "\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# The 'probability' column is a STRUCT, and 'values' contains the probabilities.\u001B[39;00m\n",
       "\u001B[0;32m---> 96\u001B[0m predictions \u001B[38;5;241m=\u001B[39m predictions\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPD\u001B[39m\u001B[38;5;124m'\u001B[39m, col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprobability\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mgetItem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mgetItem(\u001B[38;5;241m1\u001B[39m))\n",
       "\u001B[1;32m     98\u001B[0m \u001B[38;5;66;03m# Show PD for each customer\u001B[39;00m\n",
       "\u001B[1;32m     99\u001B[0m predictions\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCustomer_ID\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPD\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n",
       "\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n",
       "\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   4757\u001B[0m     )\n",
       "\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"probability\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\"."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4214444101529135>:96\u001B[0m\n\u001B[1;32m     92\u001B[0m predictions\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCustomer_ID\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprobability\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# Step 7: Extract the Probability of Default (PD) from the 'probability' column\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# The 'probability' column is a STRUCT, and 'values' contains the probabilities.\u001B[39;00m\n\u001B[0;32m---> 96\u001B[0m predictions \u001B[38;5;241m=\u001B[39m predictions\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPD\u001B[39m\u001B[38;5;124m'\u001B[39m, col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprobability\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mgetItem(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mgetItem(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     98\u001B[0m \u001B[38;5;66;03m# Show PD for each customer\u001B[39;00m\n\u001B[1;32m     99\u001B[0m predictions\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCustomer_ID\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPD\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:4758\u001B[0m, in \u001B[0;36mDataFrame.withColumn\u001B[0;34m(self, colName, col)\u001B[0m\n\u001B[1;32m   4753\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, Column):\n\u001B[1;32m   4754\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   4755\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_COLUMN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   4756\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(col)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   4757\u001B[0m     )\n\u001B[0;32m-> 4758\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwithColumn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcol\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jc\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"probability\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [INVALID_EXTRACT_BASE_FIELD_TYPE] Can't extract a value from \"probability\". Need a complex type [STRUCT, ARRAY, MAP] but got \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Create a logistic regression model in PySpark for mortgage portfolio and calculate Probability of Default (PD), Loss Given Default (LGD), Exposure at Default (EAD), and Expected Credit Loss (ECL)'''\n",
    "'''\n",
    "Steps:\n",
    "1.Create a dataset for the mortgage portfolio.\n",
    "2.Preprocess the data.\n",
    "3.Build a Logistic Regression model using PySpark.\n",
    "4.Calculate PD (Probability of Default) using the model.\n",
    "5.Calculate LGD (Loss Given Default), EAD (Exposure at Default), and ECL (Expected Credit Loss).\n",
    "Run the code on Databricks.'''\n",
    "\n",
    "\n",
    "'''\n",
    "Explanation of Key Concepts:\n",
    "Logistic Regression Model:\n",
    "\n",
    "We built a logistic regression model using various features like Credit_Score, Loan_Amount, Income, Interest_Rate, LTV, and Loan_Term to predict the likelihood of a customer defaulting on their mortgage (Default_Flag).\n",
    "PD (Probability of Default):\n",
    "\n",
    "The logistic regression model outputs a probability for each customer representing the likelihood of default (PD). A customer with a higher probability is more likely to default.\n",
    "LGD (Loss Given Default):\n",
    "\n",
    "Loss Given Default represents the portion of the loan that would be lost if the borrower defaults. It is assumed to be a function of the credit score. A low credit score leads to higher LGD.\n",
    "EAD (Exposure at Default):\n",
    "\n",
    "Exposure at Default represents the amount of loan at the time of default. In this simplified model, it is just the loan amount.\n",
    "ECL (Expected Credit Loss):\n",
    "\n",
    "ECL is calculated as PD * LGD * EAD. It is the total expected loss based on the probability of default, the loss given default, and the exposure at default.\n",
    "'''\n",
    "\n",
    "#Example Data:\n",
    "#The dataset consists of 5 customers with the following columns:\n",
    "#Customer_ID: Unique identifier for the customer.\n",
    "#Credit_Score: Credit score of the customer.\n",
    "#Loan_Amount: Amount of the mortgage loan.\n",
    "#Income: Monthly income of the customer.\n",
    "#Default_Flag: Whether the customer has defaulted (1 = default, 0 = no default).\n",
    "#Interest_Rate: Interest rate on the mortgage.\n",
    "#LTV: Loan-to-value ratio.\n",
    "#Loan_Term: Duration of the loan in years.\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf, when, lit\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"MortgageDefaultPrediction\").getOrCreate()\n",
    "\n",
    "# Step 2: Example dataset for mortgage portfolio (simplified dataset)\n",
    "data = [\n",
    "    (1, 750, 300000, 5000, 1, 0.03, 0.7, 0.9),  # Customer 1: Good credit, paid well, etc.\n",
    "    (2, 650, 150000, 3000, 0, 0.05, 0.6, 0.8),  # Customer 2: Average credit, defaulted\n",
    "    (3, 680, 250000, 4000, 1, 0.04, 0.65, 0.85), # Customer 3: Good credit, paid well, etc.\n",
    "    (4, 620, 100000, 2000, 0, 0.1, 0.5, 0.7),   # Customer 4: Poor credit, defaulted\n",
    "    (5, 700, 400000, 6000, 1, 0.02, 0.75, 0.95)  # Customer 5: Good credit, paid well\n",
    "]\n",
    "\n",
    "# Define the schema for the mortgage dataset\n",
    "columns = ['Customer_ID', 'Credit_Score', 'Loan_Amount', 'Income', 'Default_Flag', 'Interest_Rate', 'LTV', 'Loan_Term']\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the dataset\n",
    "df.show()\n",
    "\n",
    "# Step 3: Data Preprocessing: Feature Engineering\n",
    "# We will create a feature vector using Credit_Score, Loan_Amount, Income, Interest_Rate, LTV, Loan_Term\n",
    "assembler = VectorAssembler(inputCols=['Credit_Score', 'Loan_Amount', 'Income', 'Interest_Rate', 'LTV', 'Loan_Term'],\n",
    "                            outputCol='features')\n",
    "\n",
    "# Step 4: Splitting the data into training and test sets (80% training, 20% testing)\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 5: Create a Logistic Regression model\n",
    "lr = LogisticRegression(labelCol='Default_Flag', featuresCol='features')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Step 6: Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select('Customer_ID', 'prediction', 'probability').show()\n",
    "\n",
    "# Step 7: Extract the Probability of Default (PD) from the 'probability' column\n",
    "# The 'probability' column is a STRUCT, and 'values' contains the probabilities.\n",
    "predictions = predictions.withColumn('PD', col('probability').getItem('values').getItem(1))\n",
    "\n",
    "# Show PD for each customer\n",
    "predictions.select('Customer_ID', 'PD').show()\n",
    "\n",
    "# Step 8: Calculate LGD (Loss Given Default) based on Credit_Score\n",
    "# Assuming a fixed LGD rate based on the credit score.\n",
    "def calculate_LGD(credit_score):\n",
    "    if credit_score < 650:\n",
    "        return 0.9  # High loss for low credit score\n",
    "    elif credit_score < 700:\n",
    "        return 0.7  # Moderate loss for average credit score\n",
    "    else:\n",
    "        return 0.5  # Lower loss for good credit score\n",
    "\n",
    "# Register UDF to calculate LGD\n",
    "lgd_udf = udf(calculate_LGD, DoubleType())\n",
    "\n",
    "# Add LGD column based on Credit_Score (with null handling)\n",
    "predictions = predictions.withColumn('LGD', \n",
    "                                     when(col('Credit_Score').isNotNull(), lgd_udf(col('Credit_Score')))\n",
    "                                     .otherwise(lit(0)))  # Default to 0 if Credit_Score is null\n",
    "\n",
    "# Step 9: Add a column for EAD (Exposure at Default) as Loan_Amount\n",
    "predictions = predictions.withColumn('EAD', col('Loan_Amount'))\n",
    "\n",
    "# Step 10: Calculate ECL (Expected Credit Loss)\n",
    "predictions = predictions.withColumn('ECL', col('PD') * col('LGD') * col('EAD'))\n",
    "\n",
    "# Step 11: Show the final results with PD, LGD, EAD, and ECL\n",
    "predictions.select('Customer_ID', 'PD', 'LGD', 'EAD', 'ECL').show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logistic regression for pd lgd ead",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
